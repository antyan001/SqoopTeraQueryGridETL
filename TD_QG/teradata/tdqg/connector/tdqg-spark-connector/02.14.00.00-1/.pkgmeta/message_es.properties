# Connector properties
server=Servidor
port=Puerto
security=Mecanismo de autenticación
linkBufferCount=Recuento de búferes de enlace
linkBufferSize=Tamaño de búfer de enlace
linkHeartbeatInterval=Intervalo de latidos de enlace
linkHandshakeTimeout=Tiempo de espera del protocolo de enlace
readTimeout=Tiempo de espera de lectura
writeTimeout=Tiempo de espera de escritura
responseTimeout=Tiempo de espera de respuesta
importTimeout=Tiempo de espera de importación de iniciador
tempDbName=Nombre de base de datos temporal
databaseName=Nombre de la base de datos
enableLogging=Habilitar registro
defaultStringSize=Tamaño de cadena perdeterminado
defaultBinarySize=Tamaño binario predeterminado
authUserName=Nombre de usuario
authPassword=Contraseña
numExecutors=Número de ejecutores
confFilePaths=Rutas de archivo de configuración
keytab=Tabla de claves
hadoopProperties=Propiedades de Hadoop
collectActivityCount=Recopilar recuento de actividad aproximada
disablePushdown=Deshabilitar aplicación
compressionCodec=Códec de compresión
connectionMaxIdleTime=Tiempo de inactividad de conexión máximo
connectionPoolSize=Tamaño del grupo de conexiones
evictPeriodMin=Frecuencia de desalojo de conexiones
lobSupport= Compatibilidad de 16.20+ con LOB
sparkHome=Ruta de inicio de Spark
executionType=Mecanismo de ejecución de Spark
sslClientTrustStorePath=Ruta del almacén de confianza SSL
sslClientTrustStorePassword=Contraseña del almacén de confianza SSL
queueName=Nombre de cola

# Connector property description
server.description=El nombre de host del servidor de Spark Thrift
port.description=El puerto en el que escucha el servidor de Spark Thrift
security.description=El mecanismo de autenticación utilizado en el clúster
linkBufferCount.description=El número máximo de búferes para asignar por unidad de paralelismo
linkBufferSize.description=El tamaño de los buffers para asignar
linkHeartbeatInterval.description=El intervalo entre cada latido
linkHandshakeTimeout.description=El tiempo de espera del protocolo de enlace para la configuración del canal de enlace
readTimeout.description=El valor de tiempo de espera para una operación de lectura
writeTimeout.description=El valor de tiempo de espera para una operación de escritura
responseTimeout.description=El valor de tiempo de espera para recibir una respuesta
importTimeout.description=El valor de tiempo de espera para la duración de la importación del iniciador
tempDbName.description=El nombre de la base de datos temporal
databaseName.description=El nombre de la base de datos si no se especifica
enableLogging.description=El nivel de registro
defaultStringSize.description=El número de caracteres predeterminado de una columna de cadena
defaultBinarySize.description=El tamaño predeterminado de una columna binaria
authUserName.description=El nombre del usuario autorizado para seguridad
authPassword.description=La contraseña del usuario autorizado para seguridad
numExecutors.description=El número de ejecutores que se deben utilizar
confFilePaths.description=Las rutas a sitio-principal.xml, sitio-hdfs.xml y sitio-hive.xml (separadas por comas)
keytab.description=La tabla de claves que se debe usar para Kerberos
hadoopProperties.description=Propiedades de Hadoop personalizadas para reemplazar los valores de configuración del clúster
collectActivityCount.description=Si se recopila el recuento de actividad al exportar a Spark SQL (número aproximado si hay exportaciones simultáneas)
disablePushdown.description=Si se deshabilita la aplicación de predicado
compressionCodec.description=El códec de compresión que se utilizará para la exportación
connectionMaxIdleTime.description=El tiempo de inactividad máximo (en segundos) que el objeto de conexión puede permanecer en el grupo
connectionPoolSize.description=Tamaño del grupo de conexiones
evictPeriodMin.description=Frecuencia de desalojo del grupo de conexiones (en minutos)
lobSupport.description=Si las columnas STRING/BINARY se asignan a LOB en TDBMS 16.20+
sparkHome.description=Ruta al directorio de inicio de Spark, que debe incluir un subdirectorio /jars con todos los JAR de la biblioteca Spark
executionType.description=Mecanismo utilizado por el conector de destino para enviar consultas a Spark
sslClientTrustStorePath.description=Ruta del almacén de confianza SSL en los nodos del conector. Reemplaza el comportamiento predeterminado consistente en utilizar la información del almacén de claves definida en la configuración del servidor de Thrift.
sslClientTrustStorePassword.description=Contraseña del almacén de confianza SSL en los nodos del conector.
queueName.description=El nombre de la cola que envía el trabajo de Spark (solo en modo Aplicación Spark).

# Security enum names
security.NONE=Ninguno
security.KERBEROS=Kerberos

# enableLogging enum values
enableLogging.NONE=Ninguno
enableLogging.WARN=Advertir
enableLogging.INFO=Información
enableLogging.DEBUG=Depurar

# compressionCodec enum values
compressionCodec.None=Valor predeterminado del sistema
compressionCodec.Deflate=Deflate (org.apache.hadoop.io.compress.DefaultCodec)
compressionCodec.BZip2=BZip2 (org.apache.hadoop.io.compress.BZip2Codec)
compressionCodec.Gzip=Gzip (org.apache.hadoop.io.compress.GzipCodec)
compressionCodec.LZ4=LZ4 (org.apache.hadoop.io.compress.Lz4Codec)
compressionCodec.Snappy=Snappy (org.apache.hadoop.io.compress.SnappyCodec)

# Units
bytes=bytes
minutes=minutos
seconds=segundos
milliseconds=milisegundos
characters=caracteres

# executionType enum names
executionType.THRIFTSERVER=Servidor de Spark Thrift
executionType.SPARK-SUBMIT=Aplicación Spark

# Group and subgroup names
basic=Básico
securityGroup=Seguridad
queryEngine=Motor de consultas
advanced=Avanzada
ssl=Capa de sockets seguros (SSL)
fabric=Tejido
connectionPooling=Agrupación de conexiones
