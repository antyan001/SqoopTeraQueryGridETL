# Connector properties
server=服务器
port=端口
security=身份验证机制
linkBufferCount=链接缓冲区计数
linkBufferSize=链接缓冲区大小
linkHeartbeatInterval=链接检测信号间隔
linkHandshakeTimeout=链接握手超时
readTimeout=读取超时
writeTimeout=写入超时
responseTimeout=响应超时
importTimeout=发起程序导入超时
tempDbName=临时数据库名称
databaseName=数据库名称
enableLogging=启用日志记录
defaultStringSize=缺省字符串大小
defaultBinarySize=缺省二进制大小
authUserName=用户名
authPassword=密码
numExecutors=执行器数
confFilePaths=配置文件路径
keytab=Keytab
hadoopProperties=Hadoop 属性
collectActivityCount=收集近似活动计数
disablePushdown=禁用下推
compressionCodec=压缩编解码器
connectionMaxIdleTime=连接最大空闲时间
connectionPoolSize=连接池大小
evictPeriodMin=连接逐出频率
lobSupport= 16.20+ LOB 支持
sparkHome=Spark 主页路径
executionType=Spark 执行机制
sslClientTrustStorePath=SSL 信任库路径
sslClientTrustStorePassword=SSL 信任库密码
queueName=队列名称

# Connector property description
server.description=Spark Thrift 服务器的主机名
port.description=端口 Spark Thrift 服务器正在侦听
security.description=集群上使用的身份验证机制
linkBufferCount.description=每个并行单元要分配的最大缓冲区数
linkBufferSize.description=要分配的缓冲区的大小
linkHeartbeatInterval.description=每个检测信号之间的时间间隔
linkHandshakeTimeout.description=链接通道设置的握手超时
readTimeout.description=读取操作的超时值
writeTimeout.description=写入操作的超时值
responseTimeout.description=接收响应的超时值
importTimeout.description=发起程序导入持续时间的超时值
tempDbName.description=临时数据库的名称
databaseName.description=数据库的名称（如果未指定）
enableLogging.description=日志记录的级别
defaultStringSize.description=字符串列的缺省字符长度
defaultBinarySize.description=“二进制”列的缺省大小
authUserName.description=授权用户的名称，用于确保安全
authPassword.description=授权用户的密码，用于确保安全
numExecutors.description=要使用的执行器数
confFilePaths.description=core-site.xml、hdfs-site.xml 和 hive-site.xml 的路径（以逗号分隔）
keytab.description=用于 Kerberos 的 keytab
hadoopProperties.description=用于覆盖集群配置设置的自定义 hadoop 属性
collectActivityCount.description=导出到 Spark SQL 时是否收集活动计数（存在并发导出时使用估计值）
disablePushdown.description=是否禁用谓词下推
compressionCodec.description=用于导出的压缩编解码器
connectionMaxIdleTime.description=连接对象可保留在连接池中的最大空闲时间（秒）
connectionPoolSize.description=连接池大小
evictPeriodMin.description=连接池逐出频率 (分钟)
lobSupport.description=是否将 STRING/BINARY 列映射到 16.20+ TDBMS 上的 LOB
sparkHome.description=Spark 主目录的路径，Spark 库的所有 JAR 文件都应包含在该目录的 /jars 子目录内
executionType.description=向 Spark 提交查询的目标连接器使用的机制
sslClientTrustStorePath.description=连接器节点上的 SSL 信任库路径。覆盖使用 Thrift 服务器配置中定义的密钥库信息这一默认行为。
sslClientTrustStorePassword.description=连接器节点上的 SSL 信任库密码。
queueName.description=提交 Spark 作业的队列的名称 (仅限 Spark 应用程序模式)。

# Security enum names
security.NONE=无
security.KERBEROS=kerberos

# enableLogging enum values
enableLogging.NONE=无
enableLogging.WARN=警告
enableLogging.INFO=信息
enableLogging.DEBUG=调试

# compressionCodec enum values
compressionCodec.None=系统缺省值
compressionCodec.Deflate=Deflate (org.apache.hadoop.io.compress.DefaultCodec)
compressionCodec.BZip2=BZip2 (org.apache.hadoop.io.compress.BZip2Codec)
compressionCodec.Gzip=Gzip (org.apache.hadoop.io.compress.GzipCodec)
compressionCodec.LZ4=LZ4 (org.apache.hadoop.io.compress.Lz4Codec)
compressionCodec.Snappy=Snappy (org.apache.hadoop.io.compress.SnappyCodec)

# Units
bytes=字节
minutes=分钟
seconds=秒
milliseconds=毫秒
characters=字符

# executionType enum names
executionType.THRIFTSERVER=Spark Thrift 服务器
executionType.SPARK-SUBMIT=Spark 应用程序

# Group and subgroup names
basic=基本
securityGroup=安全
queryEngine=查询引擎
advanced=高级
ssl=安全套接字层 (SSL)
fabric=网络结构
connectionPooling=连接池
